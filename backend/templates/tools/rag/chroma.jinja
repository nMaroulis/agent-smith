from langchain.vectorstores import Chroma

{% if vector_store_url is defined %}
from chromadb.config import Settings
import chromadb

client = chromadb.HttpClient(host="{{ vector_store_url }}")
{{ name }}_vectorstore = Chroma(
    client=client,
    collection_name="{{ index_name }}",
    embedding_function=embeddings_model
)
{% else %}
{{ name }}_vectorstore = Chroma(
    persist_directory="{{ vector_store_path }}",
    collection_name="{{ index_name }}",
    embedding_function=embeddings_model
)
{% endif %}

{{ name }}_retriever = {{ name }}_vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={
        "k": {{ top_k }}
        {% if similarity_threshold is defined %}
        , "score_threshold": {{ similarity_threshold }}
        {% endif %}
    }
)

def {{ name }}(input):
    docs = {{ name }}_retriever.get_relevant_documents(input["question"])
    context = " ".join([doc.page_content for doc in docs])
    prompt = f"""{{ llm_followup_prompt }}""".replace("{context}", context).replace("{question}", input["question"])
    return llm.invoke(prompt)
